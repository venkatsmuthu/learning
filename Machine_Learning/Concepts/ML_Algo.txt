import nltk #toolkit
clean_tokens = ['one','two','three','four','five','one','two','three','four','one','two','three','one','two','one']
freq = nltk.FreqDist(clean_tokens)
for key,val in freq.items():
    freq.plot(20)
##    print(str(key) + ':' + str(val))



streamlit - for UI - upload file
PyPDF2-->PdfReader
langchain.textsplitter-->RecursiveCharacterTextSplitter   - creating chunks from text document
langchain.embeddings.openai -->OpenAIEmbeddings
langchain.vectorstores --> FAISS  (facebook AI Semantic Search)
langchain.chains.question_answering --> load_qa_chain
lanchain_community.chat_models --> ChatOpenAI


sklearn.model_selection -->train_test_split
sklearn.preprocessing -->StandardScaler
sklearn.linear_model --> LogisticRegression
sklearn.metrics --> accuracy_score

K-Nearest Neighbor
--------------------
-Euclidean distance
-Manhattan distance
-Hamming distance
Select K-Value where we get least mean error
Feature Scaling - to make all the features contribute equally to the model
sklearn.model_selection -->train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 0.25, random_state = 0)
sklearn.preprocessing -->StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)
sklearn.neighbors --> KNeighborsClassifier

Naive Bayes
----------
from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(x_train,y_train)

DecisionTree
---------------
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(criterion = 'entropy',max_depth = 3, random_state = 0)
model.fit(X_train,y_train)

RandomForest
---------------
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, y_train)

XGBOOST
--------


Regression:
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR


Un-Supervised Learning:
K-Means Clustering
Hierarchical clustering -- Euclidean distance, Agglomarative_Clustering
Principle Component Analysis - Dimensionality-reduction method, principle component,Eigen vector, eigen values, feature vector
from 

APIRIORI - Analogy of Bread, Butter & Ketchup  -- LHS, RHS
from apyori import apriori
rules = apriori(transactions = transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2, max_length = 2)


Reinforcement Learning:
Upper Confidence Bound - Exploration, Exploitation


